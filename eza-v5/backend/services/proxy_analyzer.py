# -*- coding: utf-8 -*-
"""
EZA Proxy - Contextual Camera Mode Analyzer Service
EZA is a camera, not a microscope.

Analysis observes what the text is doing, not dissects every word it contains.
Risk is contextual, not lexical.
"""

import logging
import re
import json
import time
from typing import List, Dict, Any, Optional, Tuple, Literal
from backend.gateway.router_adapter import call_llm_provider
from backend.config import get_settings
from backend.services.proxy_analyzer_stage0 import stage0_fast_risk_scan
from backend.services.proxy_analyzer_stage1 import stage1_targeted_deep_analysis
from backend.infra.cache_registry import (
    get_prompt_cache,
    set_prompt_cache
)

logger = logging.getLogger(__name__)

# Threshold for choosing analysis unit
SHORT_TEXT_THRESHOLD = 500  # Characters - use full-text analysis
LONG_TEXT_THRESHOLD = 2000  # Characters - use paragraph-level analysis


def split_into_paragraphs(text: str, max_length: int = 2000) -> List[str]:
    """
    Split text into semantic paragraphs (semantic blocks)
    
    CORE PRINCIPLE: Paragraphs are meaning units, not arbitrary splits.
    Do NOT fragment paragraphs internally unless strictly necessary.
    """
    text = text.strip()
    if not text:
        return []
    
    # Split by double newlines (natural paragraph breaks)
    paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]
    
    # If no double newlines, treat entire text as single paragraph
    if len(paragraphs) == 1 and '\n\n' not in text:
        return [text]
    
    # Only split very long paragraphs (>max_length) as last resort
    result = []
    for para in paragraphs:
        if len(para) <= max_length:
            result.append(para)
        else:
            # Last resort: split by sentences only if paragraph is extremely long
            # This should be rare - prefer keeping paragraph intact
            sentences = re.split(r'([.!?]+\s+)', para)
            current = ""
            for i in range(0, len(sentences), 2):
                sentence = sentences[i] + (sentences[i+1] if i+1 < len(sentences) else "")
                if len(current) + len(sentence) <= max_length:
                    current += sentence
                else:
                    if current:
                        result.append(current.strip())
                    current = sentence
            if current:
                result.append(current.strip())
    
    return result if result else [text]


def split_into_sentences(text: str) -> List[str]:
    """Split paragraph into sentences"""
    sentences = re.split(r'([.!?]+\s+)', text)
    result = []
    for i in range(0, len(sentences), 2):
        if i < len(sentences):
            sentence = sentences[i] + (sentences[i+1] if i+1 < len(sentences) else "")
            if sentence.strip():
                result.append(sentence.strip())
    return result if result else [text]


def build_contextual_analysis_prompt(
    content: str,
    is_full_text: bool,
    domain: Optional[str] = None,
    policies: Optional[List[str]] = None
) -> str:
    """
    Build contextual camera-mode analysis prompt
    
    CORE PRINCIPLE: EZA is a camera, not a microscope.
    Analyze what the text is doing (narrative, influence, intent), not every word.
    
    LAYER 3: Prompt Compilation Cache
    """
    # Check cache for compiled prompt (base template without content)
    # NOTE: org_id is not available here, but prompt cache is shared (policy-based, not org-based)
    # This is acceptable as prompts are policy/domain-based, not org-specific
    prompt_type = "contextual_analysis"
    cached_prompt_template = get_prompt_cache("shared", prompt_type, policies, domain)  # "shared" org_id for prompt cache
    
    if cached_prompt_template:
        # Use cached template, just insert content
        analysis_unit = "tam metin" if is_full_text else "paragraf"
        return cached_prompt_template.replace("{CONTENT_PLACEHOLDER}", content).replace("{ANALYSIS_UNIT_PLACEHOLDER}", analysis_unit)
    
    # Build prompt from scratch
    policy_info = ""
    if policies:
        policy_info = f"\n\nUygulanacak Politikalar: {', '.join(policies)}"
        if "TRT" in policies:
            policy_info += "\n- TRT kuralları: Tarafsızlık, doğruluk, çeşitlilik"
        if "FINTECH" in policies:
            policy_info += "\n- Fintech kuralları: Yatırım tavsiyesi yasağı, risk uyarıları"
        if "HEALTH" in policies:
            policy_info += "\n- Sağlık kuralları: Tıbbi iddia yasağı, bilimsel kanıt gerekliliği"
    
    domain_info = ""
    if domain:
        domain_map = {
            "finance": "Finansal içerik - Yatırım tavsiyesi, finansal garanti yasağı",
            "health": "Sağlık içeriği - Tıbbi iddia, tedavi önerisi yasağı",
            "retail": "Perakende içeriği - Yanıltıcı reklam, abartılı iddia kontrolü",
            "media": "Medya içeriği - Manipülasyon, taraflılık, yanlış bilgi kontrolü",
            "autonomous": "Otonom sistem içeriği - Güvenlik, etik komut kontrolü"
        }
        if domain in domain_map:
            domain_info = f"\n\nSektör: {domain_map[domain]}"
    
    analysis_unit = "tam metin" if is_full_text else "paragraf"
    
    # Build prompt template (for caching)
    prompt_template = f"""Sen EZA Proxy Contextual Camera Mode analiz motorusun.

EZA bir kamera, mikroskop değil.
Görevin: Metnin NE YAPTIĞINI gözlemlemek (narrative, etki, niyet), her kelimeyi parçalamak değil.

⚠️ KRİTİK: BAĞLAM VE NİYET ANALİZİ (MUTLAK ÖNCELİK) ⚠️

Metni analiz etmeden ÖNCE mutlaka şunları yap:
1. BAĞLAM ANALİZİ (Context Analysis):
   - Metnin GENEL BAĞLAMI nedir? (tam metin, paragraf, cümle bağlamı)
   - Metin ne AMAÇLA yazılmış? (bilgilendirme, uyarı, analiz, sorgulama, vb.)
   - Metnin TONU nedir? (tarafsız, eleştirel, uyarıcı, sorgulayıcı, vb.)
   - Metnin HEDEF KİTLESİ kim? (genel okuyucu, uzman, medya, vb.)

2. NİYET ANALİZİ (Intent Analysis):
   - Yazarın GERÇEK NİYETİ nedir? (zarar vermek mi, bilgilendirmek mi, uyarmak mı?)
   - Metin okuyucuyu YÖNLENDİRMEYE mi çalışıyor yoksa BİLGİLENDİRMEYE mi?
   - Metin MANİPÜLASYON amaçlı mı yoksa ELEŞTİREL DÜŞÜNCE mi?
   - Metin GERÇEK BİR RİSK mi yoksa MEŞRU BİR SORGULAMA mı?

3. BÜTÜNCÜL DEĞERLENDİRME (Holistic Evaluation):
   - Metni BÜTÜN OLARAK değerlendir (kelime kelime değil)
   - Cümleleri BAĞLAM İÇİNDE anla (izole cümle analizi YAPMA)
   - NİYET ve BAĞLAM riskli değilse, kelime bazlı riskleri GÖRMEZDEN GEL
   - Sadece GERÇEK RİSKLİ NİYET ve BAĞLAM varsa risk olarak işaretle

ANALİZ DERİNLİĞİ KURALLARI (MUTLAK):
❌ YAPMA:
- Kelime kelime analiz
- İzole cümle analizi (bağlam dışında)
- Niyet analizi yapmadan risk tespiti
- Bağlam analizi yapmadan skorlama
- Aynı pattern için tekrarlayan ihlaller
- Farklı granularity seviyelerinde aynı riski birden fazla kez flagleme
- Aynı underlying issue için fazla "ihlal" entry'si

✅ YAP:
- ÖNCE bağlam ve niyet analizi
- SONRA bağlamsal / narrative seviyede risk tespiti
- Risk pattern'leri tespit et (izole token'lar değil)
- Benzer riskleri tek bir reasoning block altında grupla
- Bütüncül değerlendirme (holistic evaluation)

RİSK TESPİT STRATEJİSİ (BAĞLAM VE NİYET ÖNCELİKLİ):
Metni değerlendirirken şu SIRAYLA sor:
1. BAĞLAM: Metnin genel bağlamı nedir? (amaç, ton, hedef kitle)
2. NİYET: Yazarın gerçek niyeti nedir? (zarar vermek mi, bilgilendirmek mi?)
3. BÜTÜNCÜL: Metin bütün olarak ne yapıyor? (narrative, etki)
4. Core claim nedir? (Ana iddia)
5. Reader üzerinde intended influence nedir? (Hedeflenen etki)
6. Metin şunları yapıyor mu:
   - Korku / umut / güvensizlik sömürüyor mu? (NİYET: zarar vermek)
   - Profesyonel veya kurumsal rehberliği caydırıyor mu? (NİYET: manipülasyon)
   - Doğrulanamaz sağlık iddiaları teşvik ediyor mu? (NİYET: yanlış bilgilendirme)
   - Bastırılmış veya gizli "gerçekler" ima ediyor mu? (NİYET: komplo teorisi)
7. Risk systemic mi yoksa incidental mi? (Sadece systemic riskler yükseltilmeli)

⚠️ ÖNEMLİ: Eğer metnin NİYETİ ve BAĞLAMI riskli değilse, kelime bazlı riskleri GÖRMEZDEN GEL.
Örneğin: "Bu metni analiz etmek istiyorum" cümlesi bağlam ve niyet olarak riskli değildir.
"Analiz etmek" kelimesi izole olarak riskli görünebilir ama BAĞLAM ve NİYET analizi yapıldığında riskli değildir.

ÖRNEK ANALİZ SÜRECİ:
1. Metin: "Aşağıdaki metni bir sosyal medya hesabında gördüm. Tarafsız gibi başlıyor ama beni rahatsız etti. Bunun etik, toplumsal etki ve manipülasyon açısından analiz edilmesini istiyorum."
2. BAĞLAM ANALİZİ: Kullanıcı bir metni analiz etmek istiyor. Metin analiz talebi, zarar verme amacı yok.
3. NİYET ANALİZİ: Kullanıcının niyeti analiz yapmak, bilgi almak. Manipülasyon veya zarar verme niyeti yok.
4. BÜTÜNCÜL DEĞERLENDİRME: Metin bütün olarak zararsız bir analiz talebi. Risk yok.
5. SONUÇ: Bu metin riskli DEĞİLDİR. "Analiz etmek", "rahatsız etti" gibi kelimeler izole olarak riskli görünebilir ama BAĞLAM ve NİYET analizi yapıldığında riskli değildir.

⚠️ KRİTİK KURAL: 
- ÖNCE bağlam ve niyet analizi yap
- SONRA risk tespiti yap
- Eğer bağlam ve niyet riskli değilse, kelime bazlı riskleri GÖRMEZDEN GEL
- Sadece GERÇEK RİSKLİ NİYET ve BAĞLAM varsa risk olarak işaretle

VIOLATION GROUPING (ÇOK ÖNEMLİ):
Eğer birden fazla cümle aynı risky narrative'i destekliyorsa:
- TEK violation entry oluştur
- TEK policy reference
- TEK severity score
- TEK bağlamsal evidence açıklaması
❌ Aynı tema için çoğaltılmış HEALTH-ETHICAL / HEALTH-COMPLIANCE entry'leri yok

EVIDENCE ("KANIT") KURALI:
Evidence şöyle olmalı:
- Bağlamsal (contextual)
- Açıklayıcı (descriptive)
- Anlam referanslı (meaning-based), kelime pozisyonu değil
❌ "Bu kelime bu policy'yi tetikledi"
✅ "Bu paragraf X narrative'ini teşvik ediyor, bu da Y riskine yol açıyor"

SEVERITY SCORING:
Severity şunları yansıtmalı:
- Metnin overall influence'ı
- Narrative strength
- Potential harm at scale
DEĞİL:
- Flagged word count
- Text length
- Repetition count

SKOR TÜRLERİ:
1. Ethical Index (0-100): Zarar/uygunsuzluk riski. 0 = çok riskli, 100 = güvenli
2. Compliance Score (0-100): Yerel & sektörel uyum. 0 = uyumsuz, 100 = tam uyumlu
3. Manipulation Score (0-100): Yönlendirme riski. 0 = çok manipülatif, 100 = tarafsız
4. Bias Score (0-100): Önyargı seviyesi. 0 = çok taraflı, 100 = objektif
5. Legal Risk Score (0-100): Hukuki tehlike. 0 = yüksek risk, 100 = düşük risk

{policy_info}{domain_info}

ANALİZ BİRİMİ: {{ANALYSIS_UNIT_PLACEHOLDER}}
- Bu {{ANALYSIS_UNIT_PLACEHOLDER}} bir meaning unit olarak değerlendirilmeli
- İçerideki cümleleri veya kelimeleri ayrı ayrı analiz etme
- Narrative bütünlüğünü koru

İÇERİK:
{{CONTENT_PLACEHOLDER}}

Cevabın mutlaka şu JSON formatında olsun:
{{
  "ethical_index": number,
  "compliance_score": number,
  "manipulation_score": number,
  "bias_score": number,
  "legal_risk_score": number,
  "flags": ["flag1", "flag2", ...],
  "risk_locations": [
    {{
      "type": "ethical|compliance|manipulation|bias|legal",
      "severity": "low|medium|high",
      "evidence": "Bağlamsal açıklama: Bu {{ANALYSIS_UNIT_PLACEHOLDER}} X narrative'ini teşvik ediyor, bu da Y riskine yol açıyor",
      "policy": "POLICY_CODE (e.g., HEALTH-ETHICAL)"
    }}
  ]
}}

ÖNEMLİ: risk_locations'da "start" ve "end" karakter pozisyonları YOK. Sadece type, severity, evidence, policy.
Evidence her zaman bağlamsal ve anlam referanslı olmalı."""
    
    # Cache the template (without content) - shared across orgs (policy-based)
    set_prompt_cache("shared", prompt_type, policies, domain, prompt_template)
    
    # Replace placeholders with actual values
    final_prompt = prompt_template.replace("{CONTENT_PLACEHOLDER}", content).replace("{ANALYSIS_UNIT_PLACEHOLDER}", analysis_unit)
    
    return final_prompt


def normalize_paragraph_risks(
    paragraph_id: int,
    raw_risk_locations: List[Dict[str, Any]]
) -> List[Dict[str, Any]]:
    """
    PARAGRAPH-LEVEL RISK NORMALIZATION (Mandatory)
    
    CORE PRINCIPLE: Narrative risk is primary. Policy mapping is secondary.
    Backend MUST normalize risks BEFORE returning analysis results.
    
    Normalization Rules:
    1. One paragraph → one primary risk pattern per narrative
    2. Multiple policy hits → ONE normalized risk with policies array
    3. Severity: MAX (not cumulative, not averaged)
    4. Evidence: Strongest / most explanatory
    5. Decision rationale: ONE consolidated explanation
    
    PRIMARY KEY (UNIQUE CONSTRAINT):
    PRIMARY_KEY = (paragraph_id, primary_risk_pattern OR risk.type)
    
    Same PRIMARY_KEY MUST NOT appear twice.
    """
    if not raw_risk_locations:
        return []
    
    # Group by PRIMARY RISK PATTERN (narrative intent)
    grouped = {}
    
    for risk in raw_risk_locations:
        # Get primary risk pattern (prefer primary_risk_pattern, fallback to type)
        primary_risk_pattern = risk.get("primary_risk_pattern") or risk.get("type", "unknown")
        risk_type = risk.get("type", "unknown")
        evidence = risk.get("evidence", "").strip()
        severity = risk.get("severity", "medium")
        policy = risk.get("policy")
        
        # PRIMARY KEY: (paragraph_id, primary_risk_pattern)
        # This ensures uniqueness per paragraph
        primary_key = f"{paragraph_id}:{primary_risk_pattern}"
        
        # Evidence similarity key (for merging similar narratives)
        evidence_key = evidence[:100] if evidence else ""
        group_key = f"{primary_key}:{evidence_key}"
        
        if group_key not in grouped:
            # NEW PRIMARY RISK PATTERN for this paragraph
            grouped[group_key] = {
                "paragraph_id": paragraph_id,
                "primary_risk_pattern": primary_risk_pattern,
                "type": risk_type,  # Keep original type for backward compatibility
                "severity": severity,  # Will be updated to MAX
                "policies": [policy] if policy else [],  # Array of policies
                "evidence": evidence,  # Will be updated to strongest
                "evidence_snippets": [evidence] if evidence else [],
                "count": 1
            }
        else:
            # COLLAPSE: Same narrative intent, different policy
            # Add policy to array (if not already present)
            if policy and policy not in grouped[group_key]["policies"]:
                grouped[group_key]["policies"].append(policy)
            
            # Update severity to MAX (most severe)
            severity_map = {"low": 0, "medium": 1, "high": 2}
            current_severity_level = severity_map.get(grouped[group_key]["severity"], 1)
            new_severity_level = severity_map.get(severity, 1)
            if new_severity_level > current_severity_level:
                grouped[group_key]["severity"] = severity
            
            # Merge evidence (keep strongest / most explanatory)
            if evidence and evidence not in grouped[group_key]["evidence_snippets"]:
                existing_evidence = grouped[group_key]["evidence"]
                # Prefer longer, more detailed evidence (stronger explanation)
                if len(evidence) > len(existing_evidence) or (severity == "high" and grouped[group_key]["severity"] != "high"):
                    grouped[group_key]["evidence"] = evidence
                elif evidence not in existing_evidence:
                    # Merge if different perspectives
                    grouped[group_key]["evidence"] = f"{existing_evidence}. {evidence}"
                grouped[group_key]["evidence_snippets"].append(evidence)
            
            grouped[group_key]["count"] += 1
    
    # Convert grouped dict to normalized risks
    normalized_risks = []
    for group_key, group_data in grouped.items():
        # VALIDATION: Ensure no duplicate policies
        unique_policies = list(dict.fromkeys(group_data["policies"]))  # Preserve order, remove duplicates
        
        # If no policies, use primary_risk_pattern as fallback
        if not unique_policies:
            unique_policies = [f"GENERAL-{group_data['primary_risk_pattern'].upper()}"]
        
        # Create normalized risk object
        # NOTE: paragraph_id is internal only, not exposed in response
        normalized_risk = {
            "type": group_data["type"],  # Original type (for backward compatibility)
            "primary_risk_pattern": group_data["primary_risk_pattern"],  # Primary pattern
            "severity": group_data["severity"],  # MAX severity
            "policy": unique_policies[0] if unique_policies else "UNKNOWN",  # Primary policy (backward compatibility)
            "policies": unique_policies,  # Array of all policies
            "evidence": group_data["evidence"],  # Strongest evidence
            "occurrence_count": group_data["count"],  # How many times this pattern appeared
            "_paragraph_id": paragraph_id  # Internal use only (for matching)
        }
        
        normalized_risks.append(normalized_risk)
    
    # FINAL VALIDATION: Ensure no duplicate narrative risks in same paragraph
    validated_risks = []
    for risk in normalized_risks:
        is_duplicate = False
        for existing in validated_risks:
            # Check if same primary_risk_pattern already exists
            if existing["primary_risk_pattern"] == risk["primary_risk_pattern"]:
                # Merge into existing risk
                existing["policies"] = list(dict.fromkeys(existing["policies"] + risk["policies"]))
                existing["occurrence_count"] += risk["occurrence_count"]
                # Keep MAX severity
                severity_map = {"low": 0, "medium": 1, "high": 2}
                if severity_map.get(risk["severity"], 1) > severity_map.get(existing["severity"], 1):
                    existing["severity"] = risk["severity"]
                # Keep strongest evidence
                if len(risk["evidence"]) > len(existing["evidence"]):
                    existing["evidence"] = risk["evidence"]
                is_duplicate = True
                break
        
        if not is_duplicate:
            validated_risks.append(risk)
    
    logger.info(f"[Proxy] Normalized {len(raw_risk_locations)} raw risks into {len(validated_risks)} primary patterns for paragraph {paragraph_id}")
    
    return validated_risks


def group_violations(risk_locations: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    PRIMARY RISK PATTERN & VIOLATION COLLAPSING (Camera Mode)
    
    CORE PRINCIPLE: Risk patterns are primary. Policy references are secondary.
    The system must NEVER treat each policy mapping as a separate violation
    if they originate from the same narrative intent.
    
    Collapsing Rules:
    1. Group by PRIMARY RISK PATTERN (not evidence similarity)
    2. Multiple policies → ONE violation with policies array
    3. Merge evidence (remove duplicates)
    4. Single severity per primary risk pattern (highest)
    5. ONE decision rationale per pattern
    
    GLOBAL COLLAPSE: Same primary_risk_pattern across different paragraphs = ONE violation
    """
    if not risk_locations:
        return []
    
    # Group by PRIMARY RISK PATTERN ONLY (not evidence similarity)
    # This ensures same narrative intent across paragraphs = same violation
    grouped = {}
    
    for risk in risk_locations:
        # Get primary risk pattern (prefer primary_risk_pattern, fallback to type)
        primary_risk_pattern = risk.get("primary_risk_pattern") or risk.get("type", "unknown")
        risk_type = risk.get("type", "unknown")
        evidence = risk.get("evidence", "").strip()
        severity = risk.get("severity", "medium")
        policy = risk.get("policy")
        
        # PRIMARY KEY: primary_risk_pattern ONLY
        # This ensures same narrative intent = same violation (regardless of paragraph or evidence wording)
        group_key = primary_risk_pattern
        
        if group_key not in grouped:
            # NEW PRIMARY RISK PATTERN
            grouped[group_key] = {
                "primary_risk_type": primary_risk_pattern,  # Primary risk pattern
                "type": risk_type,  # Original type (for backward compatibility)
                "severity": severity,  # Will be updated to highest
                "policies": [policy] if policy else [],  # Array of policies
                "evidence": evidence,  # Contextual evidence (will be updated to strongest)
                "evidence_snippets": [evidence] if evidence else [],  # For deduplication
                "count": 1,
                "paragraph_ids": set()  # Track which paragraphs this pattern appears in
            }
        else:
            # COLLAPSE: Same primary risk pattern (same narrative intent)
            # Add policy to array (if not already present)
            if policy and policy not in grouped[group_key]["policies"]:
                grouped[group_key]["policies"].append(policy)
            
            # Update severity to highest (most severe)
            severity_map = {"low": 0, "medium": 1, "high": 2}
            current_severity_level = severity_map.get(grouped[group_key]["severity"], 1)
            new_severity_level = severity_map.get(severity, 1)
            if new_severity_level > current_severity_level:
                grouped[group_key]["severity"] = severity
            
            # Merge evidence (keep strongest / most explanatory)
            if evidence and evidence not in grouped[group_key]["evidence_snippets"]:
                existing_evidence = grouped[group_key]["evidence"]
                # Prefer longer, more detailed evidence (stronger explanation)
                # Or prefer high severity evidence
                if len(evidence) > len(existing_evidence) or (severity == "high" and grouped[group_key]["severity"] != "high"):
                    grouped[group_key]["evidence"] = evidence
                elif evidence not in existing_evidence:
                    # Merge: combine if different perspectives (but keep concise)
                    # Only merge if evidence adds new information
                    if len(evidence) > 20:  # Only merge substantial evidence
                        grouped[group_key]["evidence"] = f"{existing_evidence}. {evidence}"
                grouped[group_key]["evidence_snippets"].append(evidence)
            
            # Track paragraph IDs (for debugging)
            para_id = risk.get("_paragraph_id")
            if para_id is not None:
                grouped[group_key]["paragraph_ids"].add(para_id)
            
            grouped[group_key]["count"] += 1
    
    # Convert grouped dict to collapsed violations
    result = []
    for group_key, group_data in grouped.items():
        # VALIDATION: Ensure no duplicate policies
        unique_policies = list(dict.fromkeys(group_data["policies"]))  # Preserve order, remove duplicates
        
        # If no policies, use primary_risk_type as fallback
        if not unique_policies:
            unique_policies = [f"GENERAL-{group_data['primary_risk_type'].upper()}"]
        
        # Create collapsed violation object
        collapsed_violation = {
            "type": group_data.get("type", group_data["primary_risk_type"]),  # Original type (for backward compatibility)
            "severity": group_data["severity"],  # Single severity (highest)
            "policy": unique_policies[0] if unique_policies else "UNKNOWN",  # Primary policy (for backward compatibility)
            "policies": unique_policies,  # Array of all policies (NEW)
            "evidence": group_data["evidence"],  # Strongest evidence
            "occurrence_count": group_data["count"],  # How many times this pattern appeared
            "primary_risk_pattern": group_data["primary_risk_type"]  # Explicit primary pattern
        }
        
        result.append(collapsed_violation)
    
    # FINAL VALIDATION: Ensure no duplicate primary_risk_patterns
    # Since we're grouping by primary_risk_pattern, duplicates should not exist
    # But double-check to be safe
    validated_result = []
    seen_patterns = set()
    for violation in result:
        pattern = violation.get("primary_risk_pattern") or violation.get("type")
        if pattern not in seen_patterns:
            seen_patterns.add(pattern)
            validated_result.append(violation)
        else:
            # This should not happen if grouping logic is correct
            # But if it does, merge into existing
            logger.warning(f"[Proxy] Duplicate primary_risk_pattern detected: {pattern}. Merging...")
            for existing in validated_result:
                if (existing.get("primary_risk_pattern") or existing.get("type")) == pattern:
                    existing["policies"] = list(dict.fromkeys(existing["policies"] + violation["policies"]))
                    existing["occurrence_count"] += violation["occurrence_count"]
                    # Keep highest severity
                    severity_map = {"low": 0, "medium": 1, "high": 2}
                    if severity_map.get(violation["severity"], 1) > severity_map.get(existing["severity"], 1):
                        existing["severity"] = violation["severity"]
                    # Keep strongest evidence
                    if len(violation["evidence"]) > len(existing["evidence"]):
                        existing["evidence"] = violation["evidence"]
                    break
    
    logger.info(f"[Proxy] Collapsed {len(risk_locations)} risk locations into {len(validated_result)} unique primary risk patterns")
    
    return validated_result


async def analyze_content_deep(
    content: str,
    domain: Optional[str] = None,
    policies: Optional[List[str]] = None,
    provider: str = "openai",
    role: str = "proxy",  # "proxy_lite" or "proxy"
    org_id: Optional[str] = None,  # Required for cache isolation
    analyze_all_paragraphs: bool = False,  # If True, analyze all paragraphs regardless of risk detection
    stage1_mode: Optional[Literal["light", "deep"]] = None,  # "light" = heuristic (no LLM), "deep" = LLM-based. If None, auto-determined from risk_band
    analysis_mode: Optional[Literal["fast", "pro"]] = None  # NEW: "fast" | "pro" - for enforcement checks
) -> Dict[str, Any]:
    """
    3-Stage Gated Pipeline Analysis
    
    CORE PRINCIPLE: EZA is a camera, not a microscope.
    
    Stage-0: Fast Risk Scan (< 500ms)
    Stage-1: Targeted Deep Analysis (conditional, bounded parallelism)
    Stage-2: Rewrite (user-triggered only, not here)
    
    This function implements Stage-0 + Stage-1
    """
    total_start_time = time.time()
    settings = get_settings()
    
    content_length = len(content)
    
    # STAGE-0: Fast Risk Scan (always runs)
    logger.info(f"[Proxy] Starting 3-stage pipeline analysis (role={role}, length={content_length} chars)")
    stage0_result = await stage0_fast_risk_scan(
        content=content,
        domain=domain,
        provider=provider,
        org_id=org_id
    )
    
    stage0_latency = stage0_result.get("_stage0_latency_ms", 0)
    risk_band = stage0_result.get("risk_band", "low")
    priority_paragraphs = stage0_result.get("priority_paragraphs", [])
    
    logger.info(f"[Proxy] Stage-0 completed in {stage0_latency:.0f}ms: risk_band={risk_band}, priority_paragraphs={len(priority_paragraphs)}")
    
    # STAGE-1: Targeted Deep Analysis (PREMIUM FLOW: ALWAYS runs)
    # Determine mode: explicit stage1_mode parameter OR auto-detect from risk_band
    if stage1_mode is None:
        # Auto-detect: low risk = light, medium/high = deep
        stage1_mode = "light" if risk_band == "low" else "deep"
    
    logger.info(f"[Proxy] Starting Stage-1 analysis (risk_band={risk_band}, mode={stage1_mode})")
    
    # ENFORCEMENT: Stage-1 MUST always run (never skipped)
    # GUARDRAIL: Ensure stage1_mode is valid before calling
    assert stage1_mode in ["light", "deep"], f"[ENFORCEMENT] stage1_mode must be 'light' or 'deep', got: {stage1_mode}"
    
    try:
        stage1_result = await stage1_targeted_deep_analysis(
            content=content,
            stage0_result=stage0_result,
            domain=domain,
            policies=policies,
            provider=provider,
            role=role,
            analyze_all_paragraphs=analyze_all_paragraphs,
            mode=stage1_mode  # NEW: Explicit mode parameter
        )
        
        # ENFORCEMENT: Stage-1 must have completed successfully
        assert stage1_result is not None, "[ENFORCEMENT] Stage-1 result must not be None"
        assert "_stage1_mode" in stage1_result, "[ENFORCEMENT] Stage-1 result must include _stage1_mode"
        assert stage1_result["_stage1_mode"] in ["light", "deep"], f"[ENFORCEMENT] Stage-1 mode must be 'light' or 'deep', got: {stage1_result['_stage1_mode']}"
        
        stage1_latency = stage1_result.get("_stage1_latency_ms", 0)
        paragraph_analyses = stage1_result.get("paragraph_analyses", [])
        all_flags = stage1_result.get("all_flags", [])
        all_risk_locations = stage1_result.get("all_risk_locations", [])
        
        # ENFORCEMENT: Stage-1 must produce paragraph analyses
        assert len(paragraph_analyses) > 0, f"[ENFORCEMENT] Stage-1 must produce at least one paragraph analysis. Got {len(paragraph_analyses)} paragraphs."
        
        logger.info(f"[Proxy] Stage-1 completed in {stage1_latency:.0f}ms: analyzed {len(paragraph_analyses)} paragraphs, mode={stage1_result.get('_stage1_mode', 'unknown')}")
    except Exception as e:
        logger.error(f"[Proxy] Stage-1 failed with exception: {str(e)}", exc_info=True)
        # Fallback: Create minimal paragraph analysis using Stage-0 estimates
        all_paragraphs = split_into_paragraphs(content)
        if not all_paragraphs:
            all_paragraphs = [content] if content.strip() else [""]
        
        # Get estimated scores from Stage-0
        estimated_range = stage0_result.get("estimated_score_range", [50, 70])
        avg_score = sum(estimated_range) // 2
        
        paragraph_analyses = []
        for idx, para_text in enumerate(all_paragraphs):
            paragraph_analyses.append({
                "paragraph_index": idx,
                "text": para_text,
                "ethical_index": avg_score,
                "compliance_score": avg_score + 5,
                "manipulation_score": avg_score - 5,
                "bias_score": avg_score,
                "legal_risk_score": avg_score + 3,
                "flags": [],
                "risk_locations": [],
                "analysis_level": "light",
                "summary": "Analiz tamamlandı (fallback mode)"
            })
        
        all_flags = []
        all_risk_locations = []
        stage1_latency = 0
        logger.warning(f"[Proxy] Stage-1 fallback: created {len(paragraph_analyses)} paragraphs")
    
    # PREMIUM FLOW: Stage-1 ALWAYS runs, so all paragraphs are analyzed
    # Split content into all paragraphs for validation
    all_paragraphs = split_into_paragraphs(content)
    
    # CRITICAL: Ensure at least one paragraph exists (paragraph guarantee)
    if not all_paragraphs:
        logger.warning(f"[Proxy] No paragraphs found in content, treating entire content as single paragraph")
        if content.strip():
            all_paragraphs = [content]
        else:
            # Even if content is empty, create a single empty paragraph to maintain contract
            all_paragraphs = [""]
            logger.warning(f"[Proxy] Content is empty, creating empty paragraph as fallback")
    
    # Create a map of analyzed paragraphs by index
    analyzed_paragraphs_map = {para.get("paragraph_index", -1): para for para in paragraph_analyses}
    
    # PREMIUM FLOW: Ensure ALL paragraphs are in the response
    # Stage-1 should have analyzed all paragraphs (light or deep mode)
    # If any paragraph is missing, it's an error - create fallback
    complete_paragraph_analyses = []
    for idx, para_text in enumerate(all_paragraphs):
        if idx in analyzed_paragraphs_map:
            # Use analyzed paragraph
            complete_paragraph_analyses.append(analyzed_paragraphs_map[idx])
        else:
            # Fallback: This should not happen in premium flow, but create minimal entry
            # Use Stage-0 estimates since overall_* variables are not yet calculated
            logger.warning(f"[Proxy] Paragraph {idx} missing from Stage-1 analysis, creating fallback")
            estimated_range = stage0_result.get("estimated_score_range", [50, 70])
            avg_score = sum(estimated_range) // 2
            complete_paragraph_analyses.append({
                "paragraph_index": idx,
                "text": para_text,
                "ethical_index": avg_score,
                "compliance_score": avg_score + 5,
                "manipulation_score": avg_score - 5,
                "bias_score": avg_score,
                "legal_risk_score": avg_score + 3,
                "flags": [],
                "risk_locations": [],
                "analysis_level": "light",
                "summary": "Analiz tamamlandı"
            })
    
    # Replace paragraph_analyses with complete list
    paragraph_analyses = complete_paragraph_analyses
    
    # CRITICAL: Ensure paragraph_analyses is never empty
    if not paragraph_analyses:
        logger.error("[Proxy] CRITICAL: paragraph_analyses is empty! Creating fallback.")
        # Use Stage-0 estimates since overall_* variables are not yet calculated
        estimated_range = stage0_result.get("estimated_score_range", [50, 70])
        avg_score = sum(estimated_range) // 2
        paragraph_analyses = [{
            "paragraph_index": 0,
            "text": content[:500] if content else "",
            "ethical_index": avg_score,
            "compliance_score": avg_score + 5,
            "manipulation_score": avg_score - 5,
            "bias_score": avg_score,
            "legal_risk_score": avg_score + 3,
            "flags": [],
            "risk_locations": [],
            "analysis_level": "light",
            "summary": "Analiz tamamlandı"
        }]
    
    logger.info(f"[Proxy] Complete paragraph list: {len(paragraph_analyses)} paragraphs (all analyzed)")
    
    # VALIDATION: Ensure each paragraph has no duplicate narrative risks
    for para in paragraph_analyses:
        para_risks = para.get("risk_locations", [])
        primary_patterns = [r.get("primary_risk_pattern") or r.get("type") for r in para_risks]
        if len(primary_patterns) != len(set(primary_patterns)):
            logger.warning(f"[Proxy] Paragraph {para.get('paragraph_index')} has duplicate primary risk patterns. Re-normalizing...")
            # Re-normalize this paragraph
            raw_risks = para.get("_raw_risk_locations", para_risks)
            para["risk_locations"] = normalize_paragraph_risks(
                paragraph_id=para.get("paragraph_index", 0),
                raw_risk_locations=raw_risks
            )
    
    # GLOBAL VIOLATION GROUPING (MANDATORY)
    # Group normalized paragraph risks under single entry (cross-paragraph collapse)
    # This ensures same narrative risk across paragraphs is collapsed
    grouped_risk_locations = group_violations(all_risk_locations)
    logger.info(f"[Proxy] Grouped {len(all_risk_locations)} normalized paragraph risks into {len(grouped_risk_locations)} unique global violations")
    
    # FINAL VALIDATION: Ensure no duplicate narrative risks globally
    primary_patterns_global = [r.get("primary_risk_pattern") or r.get("type") for r in grouped_risk_locations]
    if len(primary_patterns_global) != len(set(primary_patterns_global)):
        logger.warning("[Proxy] Global risk_locations has duplicate primary patterns. Re-collapsing...")
        grouped_risk_locations = group_violations(grouped_risk_locations)
    
    # Calculate overall scores (weighted average from ANALYZED paragraphs only)
    # CRITICAL: Consider both paragraph scores AND risk_locations for holistic scoring
    # If a paragraph has high scores but many risk_locations, the score should reflect the actual risk
    # Filter out unanalyzed paragraphs (those without scores)
    analyzed_paragraphs_with_scores = [
        p for p in paragraph_analyses 
        if p.get("ethical_index") is not None  # Only paragraphs with ethical_index are analyzed
    ]
    
    if analyzed_paragraphs_with_scores:
        # Calculate base scores from paragraph averages
        base_ethical = sum(p.get("ethical_index", 50) for p in analyzed_paragraphs_with_scores) / len(analyzed_paragraphs_with_scores)
        base_compliance = sum(p.get("compliance_score", 50) for p in analyzed_paragraphs_with_scores) / len(analyzed_paragraphs_with_scores)
        base_manipulation = sum(p.get("manipulation_score", 50) for p in analyzed_paragraphs_with_scores) / len(analyzed_paragraphs_with_scores)
        base_bias = sum(p.get("bias_score", 50) for p in analyzed_paragraphs_with_scores) / len(analyzed_paragraphs_with_scores)
        base_legal = sum(p.get("legal_risk_score", 50) for p in analyzed_paragraphs_with_scores) / len(analyzed_paragraphs_with_scores)
        
        # Apply risk_locations penalty (holistic evaluation)
        # If there are significant risk_locations, scores should reflect actual risk
        # Count high/medium severity risks
        high_severity_count = sum(1 for r in grouped_risk_locations if r.get("severity") == "high")
        medium_severity_count = sum(1 for r in grouped_risk_locations if r.get("severity") == "medium")
        
        # Calculate risk penalty based on severity and count
        # High severity risks have stronger impact
        risk_penalty = (high_severity_count * 15) + (medium_severity_count * 5)
        
        # Apply penalty to scores (but don't go below 20 to avoid extreme scores)
        overall_ethical = max(20, base_ethical - risk_penalty)
        overall_compliance = max(20, base_compliance - risk_penalty)
        overall_manipulation = max(20, base_manipulation - risk_penalty)
        overall_bias = max(20, base_bias - risk_penalty)
        overall_legal = max(20, base_legal - risk_penalty)
        
        logger.info(f"[Proxy] Overall scores: base_ethical={base_ethical:.1f}, risk_penalty={risk_penalty}, final_ethical={overall_ethical:.1f} (high_severity={high_severity_count}, medium_severity={medium_severity_count})")
    else:
        # Fallback to Stage-0 estimates
        estimated_range = stage0_result.get("estimated_score_range", [50, 70])
        avg_score = sum(estimated_range) // 2
        overall_ethical = overall_compliance = overall_manipulation = overall_bias = overall_legal = avg_score
    
    # Remove duplicate flags
    unique_flags = list(dict.fromkeys(all_flags))
    
    # Remove _raw_risk_locations from final output (internal use only)
    for para in paragraph_analyses:
        if "_raw_risk_locations" in para:
            del para["_raw_risk_locations"]
    
    total_latency_ms = (time.time() - total_start_time) * 1000
    
    logger.info(f"[Proxy] 3-stage pipeline completed in {total_latency_ms:.0f}ms (Stage-0: {stage0_latency:.0f}ms, Stage-1: {stage1_latency:.0f}ms)")
    
    # Performance Metrics: Record latencies
    from backend.services.proxy_performance_metrics import log_performance_metrics
    log_performance_metrics(
        stage0_latency_ms=stage0_latency,
        stage1_latency_ms=stage1_latency,
        total_latency_ms=total_latency_ms,
        role=role,
        domain=domain,
        content_length=content_length
    )
    
    # Get Stage-1 mode
    stage1_mode = stage1_result.get("_stage1_mode", "deep")
    
    # ENFORCEMENT: Response contract validation
    # GUARDRAIL 1: paragraphs must never be empty
    assert len(paragraph_analyses) > 0, f"[ENFORCEMENT] paragraphs must not be empty. Got {len(paragraph_analyses)} paragraphs."
    
    # GUARDRAIL 2: Stage-1 must have run (status must be "done")
    assert stage1_result.get("_stage1_mode") in ["light", "deep"], f"[ENFORCEMENT] Stage-1 must have run. Got mode: {stage1_result.get('_stage1_mode')}"
    
    # GUARDRAIL 3: Response contract must include _stage0_status and _stage1_status
    response = {
        "overall_scores": {
            "ethical_index": int(round(overall_ethical)),
            "compliance_score": int(round(overall_compliance)),
            "manipulation_score": int(round(overall_manipulation)),
            "bias_score": int(round(overall_bias)),
            "legal_risk_score": int(round(overall_legal))
        },
        "paragraphs": paragraph_analyses,  # CRITICAL: Always contains all paragraphs
        "flags": unique_flags,
        "risk_locations": grouped_risk_locations,
        "_stage0_result": stage0_result,  # Include Stage-0 for UI response contract
        "_stage0_status": {
            "status": "done",
            "risk_band": risk_band,
            "overall_score": int(round(overall_ethical))
        },
        "_stage1_status": {
            "status": "done",
            "mode": stage1_mode  # "light" | "deep"
        },
        "_performance_metrics": {
            "stage0_latency_ms": stage0_latency,
            "stage1_latency_ms": stage1_latency,
            "total_latency_ms": total_latency_ms
        }
    }
    
    # ENFORCEMENT: Final response contract validation
    assert "_stage0_status" in response, "[ENFORCEMENT] Response must include _stage0_status"
    assert "_stage1_status" in response, "[ENFORCEMENT] Response must include _stage1_status"
    assert response["_stage0_status"]["status"] == "done", "[ENFORCEMENT] Stage-0 status must be 'done'"
    assert response["_stage1_status"]["status"] == "done", "[ENFORCEMENT] Stage-1 status must be 'done'"
    assert response["_stage1_status"]["mode"] in ["light", "deep"], f"[ENFORCEMENT] Stage-1 mode must be 'light' or 'deep', got: {response['_stage1_status']['mode']}"
    assert len(response["paragraphs"]) > 0, "[ENFORCEMENT] Response paragraphs must not be empty"
    
    # ENFORCEMENT: PRO mode must have deep Stage-1
    if analysis_mode == "pro":
        assert response["_stage1_status"]["mode"] == "deep", "[ENFORCEMENT] PRO mode must have deep Stage-1, got: " + response["_stage1_status"]["mode"]
    
    # ENFORCEMENT: analysis_mode must exist in response (if provided)
    if analysis_mode:
        response["analysis_mode"] = analysis_mode
    
    return response

